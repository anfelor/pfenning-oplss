[INCLUDE=style/acmart]

Extended    : False

Anon        : False
TechReport  : True
Logo        : False
Submit      : False
Todo        : True
Tight       : False

Title       : Adjoint Functional Programming
Title Note  : Lecture notes for Frank Pfennings course at OPLSS 2024
Short Title : &Title;

Bibliography    : pfenning.bib
Math Dpi        : 300
Math Concurrency: 8
xPackage         : trfrac
Package         : graphicx
Embed           : 1000

prelinecorrection: [\setlength\preadjust{0.1em}]{input:texraw}

.supplement {
  replace: "&source;"
}

.tronly {
  display:none;
}

@techreport .tronly {
  display:block;
}

@submit .supplement {
  replace: clear;
  replace: "&source; in the tech report"
}

.intro1 {
  display: none;
}

[INCLUDE=borrowing-style]
[INCLUDE=koka-style]
[INCLUDE=graphdefs]
[INCLUDE=graphlegend]


~ HtmlOnly
[TITLE]
~

~ begin abstract
These are the lecture notes for Frank Pfenning's course at OPLSS 2024.
~ end abstract

~TexRaw
\author{Nicholas Coltharp}
\author{Anton Lorenzen}
\author{Wesley Nuzzo}
\author{Xiaotian Zhou}

\title{\mdtitle}
\maketitle
\def\shorttitle{\acmshorttitle}
~

~ tronly
~~ texraw
\makeatletter
\fancypagestyle{firstpagestyle}{%
  \fancyhf{}%
  \fancyfoot[C]{\small\thepage}%
}
\fancypagestyle{standardpagestyle}{%
  \fancyhf{}%
  \fancyfoot[C]{\small\thepage}%
}
\makeatother
\pagestyle{standardpagestyle}
~~
~

~ MathDefs
\newcommand{\gray}[1]{\colorbox{gainsboro}{\strut$#1$}}
\newcommand{\tr}[1]{\lfloor #1\rfloor}
\newcommand{\comm}{\mathbin{\triangleright}}

\newcommand\under[2]{\underset{\raisebox{2mm}{$\scriptstyle #2$}}{#1}}
\newcommand\inhx[2]{\under{#1}{\under{\uparrow}{#2}}}
\newcommand\synx[2]{\under{#1}{\under{\downarrow}{#2}}}
\newcommand\inh[1]{\under{#1}{\uparrow}}
\newcommand\syn[1]{\under{#1}{\downarrow}}
\newcommand\smallsquare{\mathbin{\vcenter{\hbox{\scalebox{0.7}{$\square$}}}}}
\newcommand\smallat{{\mkern 1mu\vcenter{\hbox{\scalebox{0.7}{@}}}\mkern-1mu}}
\newcommand{\up}[1]{\lceil #1\rceil}
~

# Lecture 2: From PL to Logic and Back ($\text{x}$2?)

We will go back and forth between logic and PL. Logic will inform our PL approach.
It is important to be aware of the connection: it is inevitable post-hoc but it
may be confusing to implement without the knowledge.

The rules from last lecture, summarized:

~ infer
\quad
------------
x : A |- x : A
~

~ begin columns 
~ begin column { width:30%; }

~ infer
\quad
------------
\cdot\, |- () : 1
~

~ infer
D |- e1 : A \quad G |- e2 : B
-----------------
D,G |- (e1, e2) : A \times B
~

~ infer
G |- e : A_k (k \in L)
-----------------
G |- k(e) : +\{ l : A_l \}_{l \in L}
~

~ end column
~ begin column

~ infer
D |- e : 1 \quad G |- e' : C
------------
D,G |- @match e @with () => e' : C
~

~ infer
D |- e : A \times B \quad G, x : A, y : B |- e' : C
---------------
D,G |- @match e @with (x,y) => e' : C
~

~ infer
D |- e : +\{ l : A_l \}_{l \in L} \quad G, x : A_l |- e_l' : C (\forall l \in L)
-------------------------
D,G |- @match e @with (l(x) => e_l')_{l \in L} : C
~

~ end column
~ end columns

We have to use every variable exactly once.

Next, we will look at an reduction relation for the language.
We want to see that there is no garbage.

In the lecture, we will see the intuition for the theorems,
but not include proofs. You can do them yourself if you want.

Dynamics: want to show type soundness. Want to prove slightly different:
also prove that there is no garbage.

We can set up a close correspondence between the static rules and the dynamic rules.
The proof will be easier if the rules are very close.

At runtime, if we have a judgement such as:

~ mathpre
G |- e : A
~

The expression $e$ is the program getting evaluated.
The type $A$ will not be carried around.
$G$ will be a variable map: $\eta : G$.

~ begin columns 
~ begin column { width:50%; }

~ infer
\quad
------------
(\cdot) : (\cdot)
~

~ end column
~ begin column

~ infer
\eta : G \quad \cdot\, |- v : A
-----------------------
\eta, x \mapsto v : (G, x : A)
~

~ end column
~ end columns

$\eta$ is an _environment_. $G$ is a _context_.

<!-- \rightsquigarrow is different -->

Under these preconditions, we want to run the program as
$\eta |- e \rightsquigarrow v$.

But splitting the context for pairs will create a problem:

~ infer
? |- e_1 \rightsquigarrow v_1 \quad ? |- e_2 \rightsquigarrow v_2
-----------------------
\eta |- (e_1, e_2) \rightsquigarrow (v_1, v_2)
~

How will we split the environment?
We can't split, because then we would always have to traverse $e1$ to see
which the variables are to figure out how to do the split. (remember that this
is evaluated at runtime).

Because this type checks: we can put $\eta$ on both sides, since we know this
will be well-formed.

One way to do this: use the subtractive approach: After $e1$ is finished, we
get back an $\eta_1$ of variables that are unused. We then pass $\eta_1$ to
the evaluation of $e2$ and get back an empty environment.
But actually, we have to do this everywhere: $e2$ returns an environment $\eta_2$,
which we return from the rule:

~ infer
\eta |- e_1 \rightsquigarrow v_1 | \eta_1 \quad \eta_1 |- e_2 \rightsquigarrow v_2 | \eta_2
-----------------------
\eta |- (e_1, e_2) \rightsquigarrow (v_1, v_2) | \eta_2
~

This also corresponds to the type checker.

Q: Is there are more logical way to do this? It seems like much gets hidden here?
A: Yes, taking some shortcuts here. In the subtractive approach:

~ infer
G |- e1 : A | D \quad D |- e2 : B | D'
-----------------------
G |- (e_1, e_2) : A \times B | D'
~

Can we write the rest of the rules now? Yes, let's look at variables:

~ infer
\quad
------------
G, x : A |- x : A | G
~

However, it is much better to do things _additive_. Subtractive has several issues:
  - It forces left-to-right evaluation, where you have to look at $e1$ before you look at $e2$.

In the additive approach: We have a context $G |- e : A | \Omega$ where $\Omega$ are the variables
that are actually used. (in contrast to the remainder as before). The pair rule becomes:

~ infer
G |- e1 : A | \Omega_1 \quad G |- e2 : B | \Omega_2
-----------------------
G |- (e1, e2) : A \times B | \Omega_1, \Omega_2
~

The result $\Omega_1, \Omega_2$ is undefined if there is any overlap between $\Omega_1$ and $\Omega_2$
(if they share a variable).

~ infer
\eta |- e_1 \rightsquigarrow v1 | \omega_1 \quad \eta |- e_2 \rightsquigarrow v2 | \omega_2
-----------------------
\eta |- (e_1, e_2) \rightsquigarrow (v_1, v_2) | (\omega_1, \omega_2)
~

If our expression type-checks then $\omega_1$ and $\omega_2$ will have disjoint domains.
If we add non-linear variables, these can occur on both sides.

Q: Where would our program get stuck if it doesn't type-check?
A: First off, not every program that doesn't type-check will get stuck.
   But if it gets stuck: the $\omega_1$ and $\omega_2$ might have overlapping domains,
   where it would get stuck.

But we have to also change our definition of environments.
What is $\eta$ in this new rule now?

In response to Q: $G$ is a typing context, but $\Omega$ just returns the used variables.

Q: Is $\Omega$ an over-approximation of the variables that are used?
A: No, we want $\Omega$ to be _exactly_ the variables used in $e$.
   We can relate our new judgement to the old one:
   Soundness: If $G |- e : A | \Omega$ then $\Omega |- e : A$ and $\Omega \subseteq G$.
   Completeness: If $\Omega |- e : A$ and $\Omega \subseteq G$, then $G |- e : A | \Omega$.

Q: If we were to set out to try and prove this, would we need two different versions of the typing rules?
A: Yes, you would show that for each derivation of one of them, you get a derivation of the other.
   This is _rule induction_.

Q: Isn't there an overapproximation in $\Omega \subseteq G$?
A: No, since our old judgement is always precise.

Q: Why can they not be the same?
A: Induction would fail in the pair rule, since even if $G = \Omega_1,\Omega_2$, then $G != \Omega_1$ or $G != \Omega_2$.

What do we want the program to satisfy? We have to change our soundness theorem:

If $G |- e : A | \Omega$ and $\eta : G$ and $\omega : \Omega$
then $\eta |- e \rightsquigarrow v | \omega$ (and $v : A$).

Since we defined them in the same way, we can now relate them in the same way.
We can prove this theorem with this kind of dynamics.

How do we know that in the end there is no garbage?

We write $\eta |- e \rightsquigarrow v | \eta$ so that everything in $\eta$ is actually used.
We can prove this for the new dynamics.

This gives us both soundness and that there will be no garbage in the end.

Q: Are the $v$ and the $\omega$ existentially quantified in this statement?
A: Yes, great question! We don't know that $e$ evaluates to $v$, since that would imply termination.
   We want to additionally quantify over the $v$ and $\omega$:

If $G |- e : A | \Omega$ and $\eta : G$ and $\eta |- e \rightsquigarrow v | \omega$,
then $\omega : \Omega$ (and $v : A$).

Q: We don't have recursion and so it should terminate?
A: Yes, that is true, but then we can't prove that using induction since termination is a stronger property.

Q: Don't we use the evaluation as a precondition now and thus can't catch stuckness?
A: Yes, this is no longer type soundness. We will use a different approach next lecture?

Q: TODO missed

Q: Can you explain what $\eta : G$ means?
A: $\eta$ is a map from variables to values. If the variable has type $A$ in $G$, then the value has type $A$ in $\eta$.

Q: Will you show a small-step semantics?
A: Not for this language, but next lecture.

Q: Does the merging extend to top-level variables?
A: Yes, treat them like non-linear variables.

A small game: how can we make this system _affine_ so that variable are used at most once?
What happens to the merge operator?

At the top-level we have to check for $G |- e : A | \Omega$ that $G = \Omega$ in the linear version to ensure that
everything in $G$ is used exactly once. In an affine setting, we can allow $\Omega \subseteq G$.

Q: It seems like the typing tree is equal to the evaluation tree?
A: Yes, since we haven't looked at interesting rules. Inviting comments: is the derivation tree the same as the evaluation tree? Student: For matches there is a difference, since we pick a branch of each match in the evaluation tree. Frank: Is there another discrepancy? After a pause: I see we have to write more rules.

~ infer
\quad
------------
\cdot\, |- () \rightsquigarrow () | \cdot
~

~ infer
\eta |- e \rightsquigarrow (v1, v2) | \omega \quad \eta, x\mapsto v1, y\mapsto v2 |- e' \rightsquigarrow v' | (\omega', x\mapsto v1, y\mapsto v2)
-----------------------
\eta |- @match e @with (x, y) => e' \rightsquigarrow v' | (\omega, \omega')
~

Let's continue.

Not much is happening in the computation. A purely linear type system, doesn't allow you to write many
interesting programs: we need top-level definitions. However, studying the whole language is very complicated,
so we study the simple case here.

Q: What is a top-level definition?
A:

```
plus (x : nat) (y : nat) : nat
plus x y = ...
```

These top-level definitions also have something important to tell us logically.
I will tell you at the end of the lecture.

Let's go back to logic: We write a natural deduction system, where $G |- A$ says that the
assumptions in $G$ can prove $A$. $D, G \coloneqq \cdot\, | G, A$. However, each assumption
needs to be used exactly once, giving us linear logic.

~ infer
D |- A \quad G |- B
--------------------
D,G |- A \otimes B
~

~ infer
D |- A \otimes B \quad G, A, B |- C
-----------------------------------
D,G |- C
~

~ infer
\quad
-------------------
A |- A
~

~ infer
G |- A
-------------------
G |- A \oplus B
~

~ infer
G |- B
-------------------
G |- A \oplus B
~

~ infer
D |- A \oplus B \quad G, A |- C \quad G, B |- C
-----------------------------------
D,G |- C
~

Historically, this was the first formulation of linear logic.
From it, people later developed linear type systems.

Our operators are:

~ mathpre
A, B \coloneqq 1 \mid A \otimes B \mid A \oplus B
~

In linear logic, there is one more operator: "of course A", written $!A$.
This allows us to reuse assumptions. We can model an ordinary function $A => B$,
as $!A -o B$ (we will introduce this formally later). Then we have judgements of
the form $\Sigma; G |- e : A$, where $\Sigma$ ahs reused hypothesis and $G$ is linear.
The full syntax is:

~ mathpre
A, B &\coloneqq 1 \mid A \otimes B \mid A \oplus B
     &\mid A -o B \mid A \& B
     &\mid !A
~

The first row is "positive" and the second row is "negative".
The first row can be duplicated by copying, eg. $1 \oplus 1 |- (1 \oplus 1) \otimes (1 \oplus 1)$,
where we duplicate the boolean $1 \oplus 1$.

Q: How do you prove this using the logic?
A: Use the sum-elimination rule, where $1 \oplus 1 |- 1 \oplus 1$.
   Then we have to show that $1 |- (1 \oplus) \otimes (1 \oplus 1)$.
   We use unit elimination so that we have to show $\cdot\, |- (1 \oplus) \otimes (1 \oplus 1)$.
   Then we use the introduction rules to obtain the term.

Lastly, we will show the rules for $A -o B$ and $A \& B$.

~ infer
G,A |- B
------------
G |- A -o B
~

We will not be able to prove $A -o (B -o A)$ in general, since $B$ is not used in the result.

~ infer
D |- A -o B \quad G |- A
-------------------------
D,G |- B
~

This is different from the rules we have seen so far, since we just plug the terms together without modifying the contexts.

How do we model this in our linear programming language? We use just one arrow $->$ and we distinguish regular from
linear functions using the variables.

~ infer
G, x : A |- e : B
-------------------
G |- \lambda x.e : A -> B
~

~ infer
D |- e1 : A -o B \quad G |- e2 : A
-------------------
D,G |- e1 e2 : B
~

You can not pattern-match against a lambda expression. This is a fundamental distinction between the positive and the negative types.
You can take a function and apply it and see what happens. Based on this, what do you think the elements of the $A \& B$ type should be?
Let's look at the logical rule:

~ infer
G |- A \quad G |- B
-------------------
G |- A \& B
~

Oh, we duplicate $G$ on both sides! How can this be sound?

~ infer
G |- A \& B
-------------------
G |- A
~

~ infer
G |- A \& B
-------------------
G |- B
~

We can only extra one of them! This is similar to the match-construct for sums.
In the programming language:

~ infer
G |- e1 : A \quad G |- e2 : B
-------------------
G |- (e1, e2) : A \& B
~

~ infer
G |- e : A \& B
-------------------
G |- e.\pi_1 : A
~

~ infer
G |- e : A \& B
-------------------
G |- e.\pi_2 : B
~

This is a _lazy pair_, we don't evaluate the components when constructing the pair. We can only 
evaluate one of them when we deconstruct the pair.
We can generalize this to $\&\{ l : A_l \}_{l \in L}$.

~ infer
G |- e_l : A_l (\forall l \in L)
-------------------
G |- (l = e_l) : \&\{ l : A_l \}_{l \in L}
~

~ infer
G |- e : \&\{ l : A_l \}_{l \in L}
-------------------
G |- e.k : A_k
~

The main takeaway: In linear logic we have positive and negative types.
We can deconstruct the positive types. We can not actually deconstruct the negative types.

We will use the lazy records for object oriented programming.