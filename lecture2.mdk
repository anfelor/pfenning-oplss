[INCLUDE=style/acmart]

Extended    : False

Anon        : False
TechReport  : True
Logo        : False
Submit      : False
Todo        : True
Tight       : False

Title       : Adjoint Functional Programming
Title Note  : Lecture notes for Frank Pfennings course at OPLSS 2024
Short Title : &Title;

Bibliography    : pfenning.bib
Math Dpi        : 300
Math Concurrency: 8
xPackage         : trfrac
Package         : graphicx
Embed           : 1000

prelinecorrection: [\setlength\preadjust{0.1em}]{input:texraw}

.supplement {
  replace: "&source;"
}

.tronly {
  display:none;
}

@techreport .tronly {
  display:block;
}

@submit .supplement {
  replace: clear;
  replace: "&source; in the tech report"
}

.intro1 {
  display: none;
}

[INCLUDE=borrowing-style]
[INCLUDE=koka-style]
[INCLUDE=graphdefs]
[INCLUDE=graphlegend]


~ HtmlOnly
[TITLE]
~

~ begin abstract
These are the lecture notes for Frank Pfenning's course at OPLSS 2024.
&bigskip;
~ end abstract

~TexRaw
\author{Nicholas Coltharp}
\author{Anton Lorenzen}
\author{Wesley Nuzzo}
\author{Xiaotian Zhou}

\title{\mdtitle}
\maketitle
\def\shorttitle{\acmshorttitle}
~

~ tronly
~~ texraw
\makeatletter
\fancypagestyle{firstpagestyle}{%
  \fancyhf{}%
  \fancyfoot[C]{\small\thepage}%
}
\fancypagestyle{standardpagestyle}{%
  \fancyhf{}%
  \fancyfoot[C]{\small\thepage}%
}
\makeatother
\pagestyle{standardpagestyle}
~~
~

~ MathDefs
\newcommand{\gray}[1]{\colorbox{gainsboro}{\strut$#1$}}
\newcommand{\tr}[1]{\lfloor #1\rfloor}
\newcommand{\comm}{\mathbin{\triangleright}}

\newcommand\under[2]{\underset{\raisebox{2mm}{$\scriptstyle #2$}}{#1}}
\newcommand\inhx[2]{\under{#1}{\under{\uparrow}{#2}}}
\newcommand\synx[2]{\under{#1}{\under{\downarrow}{#2}}}
\newcommand\inh[1]{\under{#1}{\uparrow}}
\newcommand\syn[1]{\under{#1}{\downarrow}}
\newcommand\smallsquare{\mathbin{\vcenter{\hbox{\scalebox{0.7}{$\square$}}}}}
\newcommand\smallat{{\mkern 1mu\vcenter{\hbox{\scalebox{0.7}{@}}}\mkern-1mu}}
\newcommand{\up}[1]{\lceil #1\rceil}
~

&bignegskip;

&bignegskip;

&bignegskip;

# Lecture 2: From PL to Logic and Back ($\text{x}$2?)

We will go back and forth between logic and PL. Logic will inform our PL approach.
It is important to be aware of the connection: it is inevitable post-hoc but it
may be confusing to implement without the knowledge.

The rules from last lecture, summarized:

~ infer
\quad
------------
x : A |- x : A
~

~ begin columns 
~ begin column { width:30%; }

~ infer
\quad
------------
\cdot\, |- () : 1
~

~ infer
D |- e1 : A \quad G |- e2 : B
-----------------
D,G |- (e1, e2) : A \times B
~

~ infer
G |- e : A_k (k \in L)
-----------------
G |- k(e) : +\{ l : A_l \}_{l \in L}
~

~ end column
~ begin column

~ infer
D |- e : 1 \quad G |- e' : C
------------
D,G |- @match e @with () => e' : C
~

~ infer
D |- e : A \times B \quad G, x : A, y : B |- e' : C
---------------
D,G |- @match e @with (x,y) => e' : C
~

~ infer
D |- e : +\{ l : A_l \}_{l \in L} \quad G, x : A_l |- e_l' : C (\forall l \in L)
-------------------------
D,G |- @match e @with (l(x) => e_l')_{l \in L} : C
~

~ end column
~ end columns

We have to use every variable exactly once.

## Reduction relation

In the lecture, we will see the intuition for the theorems,
but not include proofs. You can do them yourself if you want.

Next, we will look at a reduction relation for the language.
In the dynamic semantics, we want to show type soundness and also something different:
we want to show that there is no garbage at the end of the evaluation.

We can set up a close correspondence between the static rules and the dynamic rules.
The proof will be easier if the rules are very close.
At runtime, if we have a judgement such as:

~ mathpre
G |- e : A
~

The expression $e$ is the program getting evaluated.
The type $A$ will not be carried around.
$G$ will be a variable map: $\eta : G$.

~ begin columns 
~ begin column { width:50%; }

~ infer
\quad
------------
(\cdot) : (\cdot)
~

~ end column
~ begin column

~ infer
\eta : G \quad \cdot\, |- v : A
-----------------------
\eta, x \mapsto v : (G, x : A)
~

~ end column
~ end columns

$\eta$ is an _environment_. $G$ is a _context_.

Under these preconditions, we want to run the program as
$\eta\, |- e \hookrightarrow v$.
But splitting the context for pairs will create a problem:

~ infer
? |- e_1 \hookrightarrow v_1 \quad ? |- e_2 \hookrightarrow v_2
-----------------------
\eta\, |- (e_1, e_2) \hookrightarrow (v_1, v_2)
~

How will we split the environment and fill in the "?" in the rule?
We can't split $\eta$, because then we would always have to traverse $e1$
(at runtime!) to see
which the variables are to figure out how to do the split.

### The subtractive approach

However, we don't actually have to split $\eta$: under the assumption that this type checks,
we can put $\eta$ on both sides, since we know this will be well-formed.
One way to do this: use the _subtractive_ approach. After $e1$ is finished, we
get back an $\eta_1$ of variables that are unused. We then pass $\eta_1$ to
the evaluation of $e2$ and get back an empty environment.
But actually, we have to do this everywhere: $e2$ returns an environment $\eta_2$,
which we return from the rule:

~ infer
\eta\, |- e_1 \hookrightarrow v_1 \backslash \eta_1 \quad \eta_1 |- e_2 \hookrightarrow v_2 \backslash \eta_2
-----------------------
\eta\, |- (e_1, e_2) \hookrightarrow (v_1, v_2) \backslash \eta_2
~

This also corresponds to how the type checker might check that variables are only used once.

- Q: Is there are more logical way to do this? It seems like much gets hidden here?
- A: Yes, taking some shortcuts here. In the subtractive approach:

~ infer
G |- e1 : A \backslash D \quad D |- e2 : B \backslash D'
-----------------------
G |- (e_1, e_2) : A \times B \backslash D'
~

Can we write the rest of the rules now? Yes, let's look at variables:

~ infer
\quad
------------
G, x : A |- x : A \backslash G
~

### The additive approach

However, it is much better to do things _additive_. Subtractive has an issue:
It forces left-to-right evaluation, where you have to look at $e1$ before you look at $e2$.

In the additive approach: We have a context $G |- e : A \backslash \Omega$ where $\Omega$ are the variables
that are actually used. (in contrast to the remainder as before). The pair rule becomes:

~ infer
G |- e1 : A \backslash \Omega_1 \quad G |- e2 : B \backslash \Omega_2
-----------------------
G |- (e1, e2) : A \times B \backslash \Omega_1, \Omega_2
~

The result $\Omega_1, \Omega_2$ is undefined if there is any overlap between $\Omega_1$ and $\Omega_2$
(eg. if they share a variable).

~ infer
\eta\, |- e_1 \hookrightarrow v1 \backslash \omega_1 \quad \eta\, |- e_2 \hookrightarrow v2 \backslash \omega_2
-----------------------
\eta\, |- (e_1, e_2) \hookrightarrow (v_1, v_2) \backslash (\omega_1, \omega_2)
~

If our expression type-checks then $\omega_1$ and $\omega_2$ will have disjoint domains.
If we add non-linear variables, these can occur on both sides.

- Q: Where would our program get stuck if it doesn't type-check?
- A: First off, not every program that doesn't type-check will get stuck.
  But if it gets stuck: the $\omega_1$ and $\omega_2$ might have overlapping domains,
  where it would get stuck.

But we have to also change our definition of environments.
What is $\eta$ in this new rule now?

In response to Q: $G$ is a typing context, but $\Omega$ just returns the used variables.

- Q: Is $\Omega$ an over-approximation of the variables that are used?
- A: No, we want $\Omega$ to be _exactly_ the variables used in $e$.
  We can relate our new judgement to the old one:
  Soundness: If $G |- e : A \backslash \Omega$ then $\Omega |- e : A$ and $\Omega \subseteq G$.
  Completeness: If $\Omega |- e : A$ and $\Omega \subseteq G$, then $G |- e : A \backslash \Omega$.

- Q: If we were to set out to try and prove this, would we need two different versions of the typing rules?
- A: Yes, you would show that for each derivation of one of them, you get a derivation of the other.
  This is _rule induction_.

- Q: Isn't there an overapproximation in $\Omega \subseteq G$?
- A: No, since our old judgement is always precise.

- Q: Why can they not be the same?
- A: Induction would fail in the pair rule, since even if $G = \Omega_1,\Omega_2$, then $G != \Omega_1$ or $G != \Omega_2$.

### Soundness of additive approach

What do we want the program to satisfy? We have to change our soundness theorem:

If $G |- e : A \backslash \Omega$ and $\eta : G$ and $\omega : \Omega$
then $\eta\, |- e \hookrightarrow v \backslash \omega$ (and $v : A$).

Since we defined them in the same way, we can now relate them in the same way.
We can prove this theorem with this kind of dynamics.

How do we know that in the end there is no garbage?

We write $\eta\, |- e \hookrightarrow v \backslash \eta$ so that everything in $\eta$ is actually used.
We can prove this for the new dynamics.

This gives us both soundness and that there will be no garbage in the end.

 - Q: Are the $v$ and the $\omega$ existentially quantified in this statement?
 - A: Yes, great question! We don't know that $e$ evaluates to $v$, since that would imply termination.
   We want to additionally quantify over the $v$ and $\omega$:

If $G |- e : A \backslash \Omega$ and $\eta : G$ and $\eta\, |- e \hookrightarrow v \backslash \omega$,
then $\omega : \Omega$ (and $v : A$).

- Q: We don't have recursion and so it should terminate?
- A: Yes, that is true, but then we can't prove that using induction since termination is a stronger property.

- Q: Don't we use the evaluation as a precondition now and thus can't catch stuckness?
- A: Yes, this is no longer type soundness. We will use a different approach next lecture.

- Q: Can you explain what $\eta : G$ means?
- A: $\eta$ is a map from variables to values. If the variable has type $A$ in $G$, then the value has type $A$ in $\eta$.

- Q: Will you show a small-step semantics?
- A: Not for this language, but next lecture.

- Q: Does the merging extend to top-level variables?
- A: Yes, treat them like non-linear variables.

### Affine types

We can play a small game: how can we make this system _affine_ so that variable are used at most once?
What happens to the merge operator?

At the top-level we have to check for $G |- e : A \backslash \Omega$ that $G = \Omega$ in the linear version to ensure that
everything in $G$ is used exactly once. In an affine setting, we can allow $\Omega \subseteq G$.

- Q: It seems like the typing tree is equal to the evaluation tree?
- A: Yes, since we haven't looked at interesting rules. Inviting comments: is the derivation tree the same as the evaluation tree? Student: For matches there is a difference, since we pick a branch of each match in the evaluation tree. 

### Further typing rules

&nbsp;

~ infer
\quad
------------
\cdot\, |- () \hookrightarrow () \backslash \cdot
~

~ infer
\eta\, |- e \hookrightarrow (v1, v2) \backslash \omega \quad \eta, x\mapsto v1, y\mapsto v2 |- e' \hookrightarrow v' \backslash (\omega', x\mapsto v1, y\mapsto v2)
-----------------------
\eta\, |- @match e @with (x, y) => e' \hookrightarrow v' \backslash (\omega, \omega')
~

### Top-level definitions

Not much is happening in the computation. A purely linear type system, doesn't allow you to write many
interesting programs: we need top-level definitions. However, studying the whole language is very complicated,
so we study the simple case here.

 - Q: What is a top-level definition?
 - A: For example:

```
plus (x : nat) (y : nat) : nat
plus x y = ...
```

These top-level definitions also have something important to tell us logically.
I will tell you at the end of the lecture.

## Back to Logic

We write a natural deduction system, where $G |- A$ says that the
assumptions in $G$ can prove $A$. $D, G \coloneqq \cdot\, \backslash G, A$. However, each assumption
needs to be used exactly once, giving us linear logic.

~ begin columns 
~ begin column { width:30%; }

~ infer
D |- A \quad G |- B
--------------------
D,G |- A \otimes B
~

~ infer
G |- A
-------------------
G |- A \oplus B
~

~ infer
G |- B
-------------------
G |- A \oplus B
~

~ end column
~ begin column

~ infer
\quad
-------------------
A |- A
~

~ infer
D |- A \otimes B \quad G, A, B |- C
-----------------------------------
D,G |- C
~

~ infer
D |- A \oplus B \quad G, A |- C \quad G, B |- C
-----------------------------------
D,G |- C
~

~ end column
~ end columns

Historically, this was the first formulation of linear logic.
From it, people later developed linear type systems.
Our operators so far are:

~ mathpre
A, B \coloneqq 1 \mid A \otimes B \mid A \oplus B
~

### The operators of linear logic

In linear logic, there is one more operator: "of course A", written $!A$.
This allows us to reuse assumptions. We can model an ordinary function $A => B$,
as $!A -o B$ (we will introduce this formally later). Then we have judgements of
the form $\Sigma; G |- e : A$, where $\Sigma$ has reused hypothesis and $G$ is linear.
The full syntax is:

~ mathpre
A, B &\coloneqq 1 \mid A \otimes B \mid A \oplus B
     &\mid A -o B \mid A \& B
     &\mid !A
~

The first row is "positive" and the second row is "negative".
The first row can be duplicated by copying, eg. $1 \oplus 1 |- (1 \oplus 1) \otimes (1 \oplus 1)$,
where we duplicate the boolean $1 \oplus 1$.

 - Q: How do you prove this using the logic?
 - A: Use the sum-elimination rule, where $1 \oplus 1 |- 1 \oplus 1$.
   Then we have to show that $1 |- (1 \oplus) \otimes (1 \oplus 1)$.
   We use unit elimination so that we have to show $\cdot\, |- (1 \oplus) \otimes (1 \oplus 1)$.
   Then we use the introduction rules to obtain the term.

### Linear Functions

Lastly, we will show the rules for $A -o B$ and $A \& B$.

~ begin columns 
~ begin column { width:40%; }

~ infer
G,A |- B
------------
G |- A -o B
~

~ end column
~ begin column

~ infer
D |- A -o B \quad G |- A
-------------------------
D,G |- B
~

~ end column
~ end columns

We will not be able to prove $A -o (B -o A)$ in general, since $B$ is not used in the result.
This is different from the rules we have seen so far, since we just plug the terms together without modifying the contexts.

How do we model this in our linear programming language? We use just one arrow $->$ and we distinguish regular from
linear functions using the variables.

~ begin columns 
~ begin column { width:40%; }

~ infer
G, x : A |- e : B
-------------------
G |- \lambda x.e : A -> B
~

~ end column
~ begin column

~ infer
D |- e1 : A -o B \quad G |- e2 : A
-------------------
D,G |- e1 e2 : B
~

~ end column
~ end columns

You can not pattern-match against a lambda expression. This is a fundamental distinction between the positive and the negative types.

### Lazy pairs

You can not match on take a function and match on it. Instead you can only apply it and see what happens. Based on this, what do you think the elements of the $A \& B$ type should be?
Let's look at the logical rule:

~ infer
G |- A \quad G |- B
-------------------
G |- A \& B
~

Oh, we duplicate $G$ on both sides! How can this be sound?

~ begin columns 
~ begin column { width:50%; }

~ infer
G |- A \& B
-------------------
G |- A
~

~ end column
~ begin column

~ infer
G |- A \& B
-------------------
G |- B
~

~ end column
~ end columns

We can only extract one of them! This is similar to the match-construct for sums.
In the programming language:

~ begin columns 
~ begin column { width:40%; }

~ infer
G |- e1 : A \quad G |- e2 : B
-------------------
G |- (e1, e2) : A \& B
~

~ end column
~ begin column { width:30%; }

~ infer
G |- e : A \& B
-------------------
G |- e.\pi_1 : A
~

~ end column
~ begin column

~ infer
G |- e : A \& B
-------------------
G |- e.\pi_2 : B
~

~ end column
~ end columns

This is a _lazy pair_: we don't evaluate the components when constructing the pair. We can only
evaluate one of them when we deconstruct the pair.
We can generalize this to $\&\{ l : A_l \}_{l \in L}$.

~ begin columns 
~ begin column { width:60%; }

~ infer
G |- e_l : A_l (\forall l \in L)
-------------------
G |- (l = e_l) : \&\{ l : A_l \}_{l \in L}
~

~ end column
~ begin column

~ infer
G |- e : \&\{ l : A_l \}_{l \in L}
-------------------
G |- e.k : A_k
~

~ end column
~ end columns

We will use the lazy records for object oriented programming.

The main takeaway: In linear logic we have positive and negative types.
We can deconstruct the positive types. We can not actually deconstruct the negative types.
